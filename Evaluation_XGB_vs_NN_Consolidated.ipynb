{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9dadd7",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluation (XGB vs NN) — Consolidated Notebook\n",
    "\n",
    "This single notebook replaces separate variants and provides:\n",
    "- **Interactive evaluation** using `03_scored/*_with_predictions.csv` (label + `xgb_prob` + `nn_prob`).\n",
    "- **Headless artifacts ingestion** from `04_eval/` (summary/point metrics CSVs, link to latest HTML report).\n",
    "- Optional threshold selection helpers (and commented-out ipywidgets block).\n",
    "\n",
    "> No endpoint calls happen here; this notebook is presentation/analysis only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, io, re, glob, json\n",
    "from datetime import datetime\n",
    "import boto3, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             roc_curve, precision_recall_curve, confusion_matrix,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score)\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "BUCKET = os.getenv(\"BUCKET\", \"diabetes-directory\")\n",
    "SCORED_PREFIX = os.getenv(\"SCORED_PREFIX\", \"03_scored\")\n",
    "EVAL_PREFIX = os.getenv(\"EVAL_PREFIX\", \"04_eval\")\n",
    "LABEL_COL = os.getenv(\"LABEL_COL\", \"readmitted\")\n",
    "\n",
    "SESSION = boto3.session.Session(region_name=AWS_REGION)\n",
    "S3 = SESSION.client(\"s3\")\n",
    "\n",
    "print(\"Region:\", AWS_REGION, \"| Bucket:\", BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s3_list(bucket: str, prefix: str):\n",
    "    paginator = S3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            yield obj\n",
    "\n",
    "def s3_read_csv(bucket: str, key: str) -> pd.DataFrame:\n",
    "    obj = S3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "def find_latest_with_predictions(bucket: str, scored_prefix: str):\n",
    "    latest_test = None; latest_train = None\n",
    "    candidates = []\n",
    "    for obj in s3_list(bucket, f\"{scored_prefix}/\"):\n",
    "        key = obj[\"Key\"]\n",
    "        if key.lower().endswith(\"_with_predictions.csv\"):\n",
    "            candidates.append((obj[\"LastModified\"], key))\n",
    "    candidates.sort(reverse=True)\n",
    "    for _, key in candidates:\n",
    "        lname = key.lower()\n",
    "        if (latest_test is None) and (\"test\" in lname):\n",
    "            latest_test = key\n",
    "        if (latest_train is None) and (\"train\" in lname):\n",
    "            latest_train = key\n",
    "    return latest_test, latest_train\n",
    "\n",
    "TEST_KEY, TRAIN_KEY = find_latest_with_predictions(BUCKET, SCORED_PREFIX)\n",
    "print(\"Detected:\")\n",
    "print(\"  TEST_KEY :\", TEST_KEY)\n",
    "print(\"  TRAIN_KEY:\", TRAIN_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not TEST_KEY:\n",
    "    raise SystemExit(\"Could not auto-detect a test_with_predictions.csv in 03_scored/. Set TEST_KEY manually above.\")\n",
    "\n",
    "df_test = s3_read_csv(BUCKET, TEST_KEY)\n",
    "df_train = s3_read_csv(BUCKET, TRAIN_KEY) if TRAIN_KEY else None\n",
    "print(\"Test shape:\", df_test.shape, \"| Train shape:\", None if df_train is None else df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2469652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coerce_label(y: pd.Series) -> pd.Series:\n",
    "    mapping = {\"NO\":0,\"No\":0,\"no\":0,\"0\":0,\"FALSE\":0,\"False\":0,\"false\":0,\n",
    "               \"YES\":1,\"Yes\":1,\"yes\":1,\"1\":1,\"TRUE\":1,\"True\":1,\"true\":1,\n",
    "               \"<30\":1,\">30\":1}\n",
    "    if y.dtype == object:\n",
    "        y = y.map(mapping).fillna(y)\n",
    "    y = pd.to_numeric(y, errors=\"coerce\")\n",
    "    return y.astype(int)\n",
    "\n",
    "needed = [LABEL_COL, \"xgb_prob\", \"nn_prob\"]\n",
    "miss = [c for c in needed if c not in df_test.columns]\n",
    "assert not miss, f\"Missing required columns in test: {miss}\"\n",
    "\n",
    "y_t = coerce_label(df_test[LABEL_COL])\n",
    "pxgb_t = pd.to_numeric(df_test[\"xgb_prob\"], errors=\"coerce\")\n",
    "pnn_t  = pd.to_numeric(df_test[\"nn_prob\"], errors=\"coerce\")\n",
    "pens_t = (pxgb_t + pnn_t) / 2.0\n",
    "\n",
    "if df_train is not None:\n",
    "    miss_tr = [c for c in needed if c not in df_train.columns]\n",
    "    assert not miss_tr, f\"Missing required columns in train: {miss_tr}\"\n",
    "    y_tr = coerce_label(df_train[LABEL_COL])\n",
    "    pxgb_tr = pd.to_numeric(df_train[\"xgb_prob\"], errors=\"coerce\")\n",
    "    pnn_tr  = pd.to_numeric(df_train[\"nn_prob\"], errors=\"coerce\")\n",
    "    pens_tr = (pxgb_tr + pnn_tr) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_metrics(y_true, p):\n",
    "    return {\"AUC\": float(roc_auc_score(y_true, p)),\n",
    "            \"AUPRC\": float(average_precision_score(y_true, p)),\n",
    "            \"Brier\": float(brier_score_loss(y_true, p))}\n",
    "\n",
    "def point_metrics(y_true, p, thr):\n",
    "    y_pred = (p >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return {\"Accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"Precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            \"Recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            \"F1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            \"TN\": int(tn), \"FP\": int(fp), \"FN\": int(fn), \"TP\": int(tp)}\n",
    "\n",
    "def plot_roc(y_true, p, title):\n",
    "    fpr, tpr, _ = roc_curve(y_true, p)\n",
    "    auc = roc_auc_score(y_true, p)\n",
    "    plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\"); plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(title); plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_pr(y_true, p, title):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, p)\n",
    "    ap = average_precision_score(y_true, p)\n",
    "    plt.figure(); plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(title); plt.legend(loc=\"lower left\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_confusion(y_true, p, thr, title):\n",
    "    y_pred = (p >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    plt.figure(); plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"{title} (thr={thr:.2f})\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "    plt.xticks([0,1],[0,1]); plt.yticks([0,1],[0,1])\n",
    "    import numpy as np\n",
    "    for (i,j), v in np.ndenumerate(cm): plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for name, p in [(\"XGB\", pxgb_t), (\"NN\", pnn_t), (\"EnsembleAvg\", pens_t)]:\n",
    "    rows.append({\"Model\": name, **summary_metrics(y_t, p)})\n",
    "summary_test = pd.DataFrame(rows).set_index(\"Model\")\n",
    "summary_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "THRESHOLD = 0.5  # tweak and re-run\n",
    "rows = []\n",
    "for name, p in [(\"XGB\", pxgb_t), (\"NN\", pnn_t), (\"EnsembleAvg\", pens_t)]:\n",
    "    rows.append({\"Model\": name, **point_metrics(y_t, p, THRESHOLD)})\n",
    "pd.DataFrame(rows).set_index(\"Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19624cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_roc(y_t, pxgb_t, \"XGB ROC — Test\")\n",
    "plot_roc(y_t, pnn_t,  \"NN ROC — Test\")\n",
    "plot_roc(y_t, pens_t, \"Ensemble ROC — Test\")\n",
    "\n",
    "plot_pr(y_t, pxgb_t, \"XGB PR — Test\")\n",
    "plot_pr(y_t, pnn_t,  \"NN PR — Test\")\n",
    "plot_pr(y_t, pens_t, \"Ensemble PR — Test\")\n",
    "\n",
    "plot_confusion(y_t, pxgb_t, THRESHOLD, \"XGB Confusion — Test\")\n",
    "plot_confusion(y_t, pnn_t,  THRESHOLD, \"NN Confusion — Test\")\n",
    "plot_confusion(y_t, pens_t, THRESHOLD, \"Ensemble Confusion — Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08495002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if df_train is not None:\n",
    "    rows = []\n",
    "    for name, p in [(\"XGB\", pxgb_tr), (\"NN\", pnn_tr), (\"EnsembleAvg\", pens_tr)]:\n",
    "        rows.append({\"Model\": name, **summary_metrics(y_tr, p)})\n",
    "    display(pd.DataFrame(rows).set_index(\"Model\"))\n",
    "    rows = []\n",
    "    for name, p in [(\"XGB\", pxgb_tr), (\"NN\", pnn_tr), (\"EnsembleAvg\", pens_tr)]:\n",
    "        rows.append({\"Model\": name, **point_metrics(y_tr, p, THRESHOLD)})\n",
    "    display(pd.DataFrame(rows).set_index(\"Model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimal_thresholds(y_true, p):\n",
    "    import numpy as np\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    thr = np.append(thr, 1.0)\n",
    "    f1s = 2 * (prec * rec) / np.maximum(prec + rec, 1e-12)\n",
    "    thr_f1 = thr[np.nanargmax(f1s)]\n",
    "    fpr, tpr, thr2 = roc_curve(y_true, p)\n",
    "    J = tpr - fpr\n",
    "    thr_j = thr2[np.nanargmax(J)]\n",
    "    return {\"F1_opt\": float(thr_f1), \"YoudenJ_opt\": float(thr_j)}\n",
    "\n",
    "print(\"Optimal thresholds (TEST):\")\n",
    "print(\"XGB:\", optimal_thresholds(y_t, pxgb_t))\n",
    "print(\"NN :\", optimal_thresholds(y_t, pnn_t))\n",
    "print(\"ENS:\", optimal_thresholds(y_t, pens_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbe82b",
   "metadata": {},
   "source": [
    "## Load headless artifacts from `04_eval/` (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_latest_eval_tables(bucket: str, eval_prefix: str):\n",
    "    latest_summary = None; latest_points = None; latest_report = None\n",
    "    cand = []\n",
    "    for obj in s3_list(bucket, f\"{eval_prefix}/\"):\n",
    "        key = obj[\"Key\"]\n",
    "        lname = key.lower()\n",
    "        if lname.endswith(\".csv\") or lname.endswith(\".html\"):\n",
    "            cand.append((obj[\"LastModified\"], key))\n",
    "    cand.sort(reverse=True)\n",
    "    for _, key in cand:\n",
    "        if key.endswith(\".csv\") and \"summary_\" in key and latest_summary is None:\n",
    "            latest_summary = key\n",
    "        if key.endswith(\".csv\") and \"point_metrics_\" in key and latest_points is None:\n",
    "            latest_points = key\n",
    "        if key.endswith(\".html\") and \"report_\" in key and latest_report is None:\n",
    "            latest_report = key\n",
    "        if latest_summary and latest_points and latest_report:\n",
    "            break\n",
    "    return latest_summary, latest_points, latest_report\n",
    "\n",
    "SUM_KEY, POINT_KEY, REPORT_KEY = find_latest_eval_tables(BUCKET, EVAL_PREFIX)\n",
    "print(\"Latest eval artifacts:\")\n",
    "print(\"  summary_csv :\", SUM_KEY)\n",
    "print(\"  points_csv  :\", POINT_KEY)\n",
    "print(\"  html_report :\", REPORT_KEY)\n",
    "\n",
    "if SUM_KEY:\n",
    "    df_sum = s3_read_csv(BUCKET, SUM_KEY)\n",
    "    display(df_sum.head())\n",
    "if POINT_KEY:\n",
    "    df_pts = s3_read_csv(BUCKET, POINT_KEY)\n",
    "    display(df_pts.head())\n",
    "if REPORT_KEY:\n",
    "    print(\"Open in browser:\", f\"https://{BUCKET}.s3.amazonaws.com/{REPORT_KEY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional interactive slider (uncomment if ipywidgets is installed)\n",
    "# from ipywidgets import interact, FloatSlider\n",
    "# def show_at_threshold(thr=0.5):\n",
    "#     rows = []\n",
    "#     for name, p in [(\"XGB\", pxgb_t), (\"NN\", pnn_t), (\"EnsembleAvg\", pens_t)]:\n",
    "#         rows.append({\"Model\": name, **point_metrics(y_t, p, thr)})\n",
    "#     display(pd.DataFrame(rows).set_index(\"Model\"))\n",
    "#     plot_confusion(y_t, pxgb_t, thr, \"XGB Confusion — Test\")\n",
    "#     plot_confusion(y_t, pnn_t,  thr, \"NN Confusion — Test\")\n",
    "#     plot_confusion(y_t, pens_t, thr, \"Ensemble Confusion — Test\")\n",
    "# interact(show_at_threshold, thr=FloatSlider(min=0.01, max=0.99, step=0.01, value=0.5))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
